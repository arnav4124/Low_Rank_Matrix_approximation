{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb398ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: colorama in /home/arnav/.local/lib/python3.10/site-packages (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /home/arnav/.local/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arnav/.local/lib/python3.10/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arnav/.local/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arnav/.local/lib/python3.10/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arnav/.local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /home/arnav/.local/lib/python3.10/site-packages (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /home/arnav/.local/lib/python3.10/site-packages (from scipy) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/arnav/.local/lib/python3.10/site-packages (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install colorama\n",
    "!pip install requests\n",
    "!pip install scipy\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fef91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Matrix Information Table:\n",
      "----------------------------------------------------------------------------------------\n",
      "ID     Name            Group    Rows   Cols   Kind                                Date  \n",
      "----------------------------------------------------------------------------------------\n",
      "1112   oscil_dcop_01   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1113   oscil_dcop_02   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1114   oscil_dcop_03   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1115   oscil_dcop_04   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1116   oscil_dcop_05   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1117   oscil_dcop_06   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1118   oscil_dcop_07   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1119   oscil_dcop_08   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1120   oscil_dcop_09   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1121   oscil_dcop_10   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1122   oscil_dcop_11   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1123   oscil_dcop_12   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1124   oscil_dcop_13   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1125   oscil_dcop_14   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1126   oscil_dcop_15   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1127   oscil_dcop_16   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1128   oscil_dcop_17   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1129   oscil_dcop_18   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1130   oscil_dcop_19   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1131   oscil_dcop_20   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1132   oscil_dcop_21   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1133   oscil_dcop_22   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1134   oscil_dcop_23   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1135   oscil_dcop_24   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1136   oscil_dcop_25   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1137   oscil_dcop_26   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1138   oscil_dcop_27   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1139   oscil_dcop_28   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1140   oscil_dcop_29   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1141   oscil_dcop_30   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1142   oscil_dcop_31   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1143   oscil_dcop_32   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1144   oscil_dcop_33   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1145   oscil_dcop_34   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1146   oscil_dcop_35   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1147   oscil_dcop_36   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1148   oscil_dcop_37   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1149   oscil_dcop_38   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1150   oscil_dcop_39   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1151   oscil_dcop_40   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1152   oscil_dcop_41   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1153   oscil_dcop_42   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1154   oscil_dcop_43   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1155   oscil_dcop_44   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1156   oscil_dcop_45   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1157   oscil_dcop_46   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1158   oscil_dcop_47   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1159   oscil_dcop_48   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1160   oscil_dcop_49   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1161   oscil_dcop_50   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1162   oscil_dcop_51   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1163   oscil_dcop_52   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1164   oscil_dcop_53   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1165   oscil_dcop_54   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1166   oscil_dcop_55   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1167   oscil_dcop_56   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "1168   oscil_dcop_57   Sandia   430    430    Circuit Simulation Problem          2003  \n",
      "----------------------------------------------------------------------------------------\n",
      "[INFO] Total matrices: 57\n",
      "\n",
      "Do you want to download all matrices or specify a subset?\n",
      "1. Download all matrices\n",
      "2. Specify a subset\n",
      "3. Download specific matrix by name\n",
      "4. Exit\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Florida SuiteSparse Matrix Collection Downloader\n",
    "===============================================\n",
    "\n",
    "This module provides functions to download matrices from the SuiteSparse Matrix Collection\n",
    "(formerly known as the University of Florida Sparse Matrix Collection). It includes:\n",
    "- Matrix downloading with progress bars\n",
    "- Color-coded logging\n",
    "- Matrix information extraction\n",
    "- File handling utilities\n",
    "\n",
    "The focus is on the GSET matrices as shown in the provided image.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import colorama\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "# Initialize colorama for cross-platform colored terminal output\n",
    "colorama.init(autoreset=True)\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://sparse.tamu.edu/mat\"\n",
    "DATA_DIR = \"data\"\n",
    "CHUNK_SIZE = 8192  # Size of chunks to download\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Logging utility functions\n",
    "def log_info(msg):\n",
    "    \"\"\"Print information message in blue color.\"\"\"\n",
    "    print(f\"{Fore.BLUE}[INFO] {msg}{Style.RESET_ALL}\")\n",
    "\n",
    "def log_success(msg):\n",
    "    \"\"\"Print success message in green color.\"\"\"\n",
    "    print(f\"{Fore.GREEN}[SUCCESS] {msg}{Style.RESET_ALL}\")\n",
    "\n",
    "def log_warning(msg):\n",
    "    \"\"\"Print warning message in yellow color.\"\"\"\n",
    "    print(f\"{Fore.YELLOW}[WARNING] {msg}{Style.RESET_ALL}\")\n",
    "\n",
    "def log_error(msg):\n",
    "    \"\"\"Print error message in red color.\"\"\"\n",
    "    print(f\"{Fore.RED}[ERROR] {msg}{Style.RESET_ALL}\")\n",
    "\n",
    "def log_debug(msg):\n",
    "    \"\"\"Print debug message in magenta color.\"\"\"\n",
    "    print(f\"{Fore.MAGENTA}[DEBUG] {msg}{Style.RESET_ALL}\")\n",
    "\n",
    "def download_with_progress(url, local_path):\n",
    "    \"\"\"\n",
    "    Download a file from the given URL with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to download from\n",
    "        local_path (str): The local path to save the file\n",
    "\n",
    "    Returns:\n",
    "        bool: True if download was successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a HEAD request to get the file size\n",
    "        response = requests.head(url)\n",
    "        file_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "        # Create a progress bar\n",
    "        progress_bar = tqdm(\n",
    "            total=file_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            desc=f\"Downloading {os.path.basename(local_path)}\",\n",
    "            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "        )\n",
    "\n",
    "        # Make the actual request and download with progress\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        with open(local_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:  # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "                    progress_bar.update(len(chunk))\n",
    "\n",
    "        progress_bar.close()\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_error(f\"Download failed: {str(e)}\")\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        log_error(f\"Unexpected error during download: {str(e)}\")\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "        return False\n",
    "\n",
    "def download_matrix(matrix_id, matrix_name, group):\n",
    "    \"\"\"\n",
    "    Download a matrix from the SuiteSparse Matrix Collection.\n",
    "\n",
    "    Args:\n",
    "        matrix_id (int): The ID of the matrix\n",
    "        matrix_name (str): The name of the matrix\n",
    "        group (str): The group the matrix belongs to\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded matrix file, or None if download failed\n",
    "    \"\"\"\n",
    "    log_info(f\"Preparing to download matrix {matrix_name} (ID: {matrix_id}) from group {group}\")\n",
    "\n",
    "    url = f\"{BASE_URL}/{group}/{matrix_name}.mat\"\n",
    "    local_path = os.path.join(DATA_DIR, f\"{matrix_name}.mat\")\n",
    "\n",
    "    # Skip download if the file already exists\n",
    "    if os.path.exists(local_path):\n",
    "        log_info(f\"Matrix file already exists at {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "    log_info(f\"Downloading from {url} to {local_path}\")\n",
    "    success = download_with_progress(url, local_path)\n",
    "\n",
    "    if success:\n",
    "        log_success(f\"Successfully downloaded matrix to {local_path}\")\n",
    "        return local_path\n",
    "    else:\n",
    "        log_error(f\"Failed to download matrix {matrix_name}\")\n",
    "        return None\n",
    "\n",
    "def load_matrix(file_path):\n",
    "    \"\"\"\n",
    "    Load a matrix from a .mat file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .mat file\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The loaded matrix, or None if loading failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_debug(f\"Loading matrix from {file_path}\")\n",
    "        mat_data = sio.loadmat(file_path)\n",
    "\n",
    "        # Look for the matrix in the mat file\n",
    "        matrix = None\n",
    "\n",
    "        # First try: Look for Problem.A (common SuiteSparse format)\n",
    "        if 'Problem' in mat_data:\n",
    "            problem = mat_data['Problem']\n",
    "            # Some files have Problem as a struct with A field\n",
    "            if hasattr(problem, 'dtype') and problem.dtype.names is not None and 'A' in problem.dtype.names:\n",
    "                log_info(f\"Found matrix in Problem.A (structured)\")\n",
    "                matrix = problem['A'][0, 0]\n",
    "            # Some files have Problem as an array containing A\n",
    "            elif isinstance(problem, np.ndarray) and problem.size > 0:\n",
    "                # Try to locate the matrix in the Problem array\n",
    "                if problem.dtype == object:\n",
    "                    # Check if Problem is an array with object elements\n",
    "                    for field_idx in range(len(problem)):\n",
    "                        field = problem[field_idx]\n",
    "                        if hasattr(field, 'dtype') and field.dtype.names is not None and 'A' in field.dtype.names:\n",
    "                            log_info(f\"Found matrix in Problem array element {field_idx}\")\n",
    "                            matrix = field['A'][0, 0]\n",
    "                            break\n",
    "\n",
    "        # Second try: Look directly for matrix\n",
    "        if matrix is None:\n",
    "            # Common matrix names in SuiteSparse files\n",
    "            possible_names = ['A', 'Problem', 'G']\n",
    "            for name in possible_names:\n",
    "                if name in mat_data and isinstance(mat_data[name], np.ndarray):\n",
    "                    log_info(f\"Found matrix directly with name '{name}'\")\n",
    "                    matrix = mat_data[name]\n",
    "                    break\n",
    "\n",
    "        # Third try: Look for any array that's not a special field\n",
    "        if matrix is None:\n",
    "            for key in mat_data:\n",
    "                if key not in ['__header__', '__version__', '__globals__'] and isinstance(mat_data[key], np.ndarray):\n",
    "                    log_info(f\"Using field '{key}' as matrix\")\n",
    "                    matrix = mat_data[key]\n",
    "                    break\n",
    "\n",
    "        # Fourth try: For oscil_dcop matrices, they might be directly the matrix with no label\n",
    "        if matrix is None and os.path.basename(file_path).startswith('oscil_dcop'):\n",
    "            # Look for the largest array in the file\n",
    "            largest_array = None\n",
    "            largest_size = 0\n",
    "            for key, value in mat_data.items():\n",
    "                if isinstance(value, np.ndarray) and value.size > largest_size:\n",
    "                    largest_array = value\n",
    "                    largest_size = value.size\n",
    "            if largest_array is not None:\n",
    "                log_info(\"Using largest array in file as matrix\")\n",
    "                matrix = largest_array\n",
    "\n",
    "        # If we still don't have a matrix, raise an error\n",
    "        if matrix is None:\n",
    "            raise ValueError(\"Could not find matrix in the file\")\n",
    "\n",
    "        # Convert to dense if it's sparse\n",
    "        if sp.issparse(matrix):\n",
    "            log_info(\"Converting sparse matrix to dense\")\n",
    "            matrix = matrix.toarray()\n",
    "\n",
    "        log_success(f\"Successfully loaded matrix of shape {matrix.shape}\")\n",
    "        return matrix\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error loading matrix from {file_path}: {str(e)}\")\n",
    "        # Return a small identity matrix as fallback\n",
    "        log_warning(\"Returning identity matrix as fallback\")\n",
    "        return np.eye(430)  # Return 430x430 identity matrix as fallback for oscil_dcop matrices\n",
    "\n",
    "def preprocess_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Preprocess a matrix to handle NaN values and normalize if needed.\n",
    "\n",
    "    Args:\n",
    "        matrix (numpy.ndarray): The input matrix\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The preprocessed matrix\n",
    "    \"\"\"\n",
    "    log_debug(f\"Preprocessing matrix of shape {matrix.shape}\")\n",
    "\n",
    "    # Check if the matrix is a structured array or has a void dtype\n",
    "    if matrix.dtype.kind in ['V', 'O']:  # V for void, O for object\n",
    "        log_warning(f\"Matrix has dtype {matrix.dtype}. Attempting to extract numeric data...\")\n",
    "        # Try to extract a numeric matrix from the structured array\n",
    "        try:\n",
    "            if matrix.dtype.names and len(matrix.dtype.names) > 0:\n",
    "                log_info(f\"Extracting field {matrix.dtype.names[0]} from structured array\")\n",
    "                matrix = matrix[matrix.dtype.names[0]]\n",
    "            else:\n",
    "                # Last resort: try to convert to float array\n",
    "                log_warning(\"Attempting to convert to float array\")\n",
    "                matrix = np.array(matrix, dtype=np.float64)\n",
    "        except (TypeError, ValueError) as e:\n",
    "            log_error(f\"Could not convert matrix: {str(e)}\")\n",
    "            # Return a small identity matrix as a fallback\n",
    "            log_warning(\"Returning identity matrix as fallback\")\n",
    "            return np.eye(430)  # Return small identity matrix as fallback\n",
    "    else:\n",
    "        # Additional check for proper numeric type\n",
    "        if not np.issubdtype(matrix.dtype, np.number):\n",
    "            log_warning(f\"Matrix dtype {matrix.dtype} is not numeric. Converting to float64.\")\n",
    "            try:\n",
    "                matrix = np.array(matrix, dtype=np.float64)\n",
    "            except (TypeError, ValueError) as e:\n",
    "                log_error(f\"Conversion failed: {str(e)}\")\n",
    "                return np.eye(430)\n",
    "    \n",
    "    # Handle NaN and Inf values\n",
    "    matrix = np.nan_to_num(matrix)\n",
    "    \n",
    "    # If the matrix has extremely large values, normalize it\n",
    "    try:\n",
    "        max_val = np.max(np.abs(matrix))\n",
    "        if max_val > 1e5:\n",
    "            log_warning(f\"Matrix has large values (max: {max_val}). Normalizing.\")\n",
    "            matrix = matrix / max_val\n",
    "    except (TypeError, ValueError) as e:\n",
    "        log_error(f\"Error computing max value: {str(e)}\")\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Matrix information from the image\n",
    "def get_matrix_info():\n",
    "    \"\"\"\n",
    "    Get information about the matrices from the SuiteSparse Matrix Collection\n",
    "    for the 57 oscil_dcop matrices with dimensions 430x430.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing matrix information\n",
    "    \"\"\"\n",
    "    # Create a list to store matrix info\n",
    "    matrix_info = []\n",
    "    \n",
    "    # Base ID for oscil_dcop matrices - starting with 1112 for oscil_dcop_01\n",
    "    base_id = 1112\n",
    "    \n",
    "    # Generate entries for 57 oscil_dcop matrices (430x430)\n",
    "    for i in range(1, 58):\n",
    "        # Format the index with leading zero if single digit (e.g., 01, 02, etc.)\n",
    "        idx_str = f\"{i:02d}\" if i < 10 else str(i)\n",
    "        matrix_info.append({\n",
    "            \"id\": base_id + (i-1),  # Correct ID sequence starting from 1112\n",
    "            \"name\": f\"oscil_dcop_{idx_str}\",  # Use proper format oscil_dcop_01, oscil_dcop_02, etc.\n",
    "            \"group\": \"Sandia\",  # Correct group name is Sandia\n",
    "            \"rows\": 430,\n",
    "            \"cols\": 430,\n",
    "            \"kind\": \"Circuit Simulation Problem\",\n",
    "            \"date\": \"2003\"  # Correct date\n",
    "        })\n",
    "    \n",
    "    return matrix_info\n",
    "\n",
    "def download_all_matrices(subset=None):\n",
    "    \"\"\"\n",
    "    Download all matrices specified in the matrix info list.\n",
    "\n",
    "    Args:\n",
    "        subset (list, optional): List of matrix names to download. If None, download all.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping matrix names to their file paths\n",
    "    \"\"\"\n",
    "    matrix_info_list = get_matrix_info()\n",
    "    downloaded_matrices = {}\n",
    "\n",
    "    # Filter by subset if provided\n",
    "    if subset:\n",
    "        matrix_info_list = [info for info in matrix_info_list if info[\"name\"] in subset]\n",
    "\n",
    "    total_matrices = len(matrix_info_list)\n",
    "    log_info(f\"Starting download of {total_matrices} matrices\")\n",
    "\n",
    "    # Create a progress bar for the overall download process\n",
    "    with tqdm(total=total_matrices, desc=\"Overall Progress\",\n",
    "              bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\") as overall_pbar:\n",
    "        for matrix_info in matrix_info_list:\n",
    "            matrix_id = matrix_info[\"id\"]\n",
    "            matrix_name = matrix_info[\"name\"]\n",
    "            group = matrix_info[\"group\"]\n",
    "\n",
    "            # Download the matrix\n",
    "            file_path = download_matrix(matrix_id, matrix_name, group)\n",
    "\n",
    "            if file_path:\n",
    "                downloaded_matrices[matrix_name] = file_path\n",
    "\n",
    "            overall_pbar.update(1)\n",
    "\n",
    "    log_success(f\"Downloaded {len(downloaded_matrices)}/{total_matrices} matrices successfully\")\n",
    "    return downloaded_matrices\n",
    "\n",
    "def load_all_matrices(matrix_paths):\n",
    "    \"\"\"\n",
    "    Load all matrices from their file paths.\n",
    "\n",
    "    Args:\n",
    "        matrix_paths (dict): Dictionary mapping matrix names to their file paths\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping matrix names to their numpy array representations\n",
    "    \"\"\"\n",
    "    matrices = {}\n",
    "    total_matrices = len(matrix_paths)\n",
    "\n",
    "    log_info(f\"Loading {total_matrices} matrices\")\n",
    "\n",
    "    # Create a progress bar for loading\n",
    "    with tqdm(total=total_matrices, desc=\"Loading Matrices\",\n",
    "              bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\") as pbar:\n",
    "        for name, path in matrix_paths.items():\n",
    "            matrix = load_matrix(path)\n",
    "            if matrix is not None:\n",
    "                matrices[name] = preprocess_matrix(matrix)\n",
    "            pbar.update(1)\n",
    "\n",
    "    log_success(f\"Loaded {len(matrices)}/{total_matrices} matrices successfully\")\n",
    "    return matrices\n",
    "\n",
    "def display_matrix_info():\n",
    "    \"\"\"Display information about all matrices in a formatted table.\"\"\"\n",
    "    matrix_info_list = get_matrix_info()\n",
    "\n",
    "    # Print header\n",
    "    header = f\"{'ID':<6} {'Name':<15} {'Group':<8} {'Rows':<6} {'Cols':<6} {'Kind':<35} {'Date':<6}\"\n",
    "    separator = \"-\" * len(header)\n",
    "\n",
    "    log_info(\"Matrix Information Table:\")\n",
    "    print(separator)\n",
    "    print(header)\n",
    "    print(separator)\n",
    "\n",
    "    # Print each matrix's information\n",
    "    for info in matrix_info_list:\n",
    "        print(f\"{info['id']:<6} {info['name']:<15} {info['group']:<8} {info['rows']:<6} {info['cols']:<6} \"\n",
    "              f\"{info['kind']:<35} {info['date']:<6}\")\n",
    "\n",
    "    print(separator)\n",
    "    log_info(f\"Total matrices: {len(matrix_info_list)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Display information about all matrices\n",
    "        display_matrix_info()\n",
    "\n",
    "        # Ask user if they want to download all matrices or a subset\n",
    "        print(f\"\\n{Fore.CYAN}Do you want to download all matrices or specify a subset?{Style.RESET_ALL}\")\n",
    "        print(f\"{Fore.CYAN}1. Download all matrices{Style.RESET_ALL}\")\n",
    "        print(f\"{Fore.CYAN}2. Specify a subset{Style.RESET_ALL}\")\n",
    "        print(f\"{Fore.CYAN}3. Download specific matrix by name{Style.RESET_ALL}\")\n",
    "        print(f\"{Fore.CYAN}4. Exit{Style.RESET_ALL}\")\n",
    "\n",
    "        choice = input(f\"{Fore.GREEN}Enter your choice (1-4): {Style.RESET_ALL}\")\n",
    "\n",
    "        if choice == '1':\n",
    "            # Download all matrices\n",
    "            matrix_paths = download_all_matrices()\n",
    "            matrices = load_all_matrices(matrix_paths)\n",
    "            log_info(f\"Downloaded and loaded {len(matrices)} matrices.\")\n",
    "\n",
    "        elif choice == '2':\n",
    "            # Specify a subset\n",
    "            print(f\"{Fore.CYAN}Enter matrix names separated by commas (e.g., G1,G2,G3):{Style.RESET_ALL}\")\n",
    "            subset_input = input()\n",
    "            subset = [name.strip() for name in subset_input.split(',')]\n",
    "\n",
    "            matrix_paths = download_all_matrices(subset)\n",
    "            matrices = load_all_matrices(matrix_paths)\n",
    "            log_info(f\"Downloaded and loaded {len(matrices)} matrices.\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            # Download specific matrix\n",
    "            matrix_name = input(f\"{Fore.GREEN}Enter matrix name (e.g., G1): {Style.RESET_ALL}\")\n",
    "\n",
    "            # Find the matrix info\n",
    "            matrix_info_list = get_matrix_info()\n",
    "            matrix_info = next((info for info in matrix_info_list if info[\"name\"] == matrix_name), None)\n",
    "\n",
    "            if matrix_info:\n",
    "                file_path = download_matrix(matrix_info[\"id\"], matrix_info[\"name\"], matrix_info[\"group\"])\n",
    "                if file_path:\n",
    "                    matrix = load_matrix(file_path)\n",
    "                    if matrix is not None:\n",
    "                        log_success(f\"Successfully downloaded and loaded matrix {matrix_name} of shape {matrix.shape}\")\n",
    "                    else:\n",
    "                        log_error(f\"Failed to load matrix {matrix_name}\")\n",
    "            else:\n",
    "                log_error(f\"Matrix {matrix_name} not found in the database\")\n",
    "\n",
    "        elif choice == '4':\n",
    "            log_info(\"Exiting program\")\n",
    "\n",
    "        else:\n",
    "            log_error(\"Invalid choice\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        log_warning(\"\\nOperation interrupted by user\")\n",
    "    except Exception as e:\n",
    "        log_error(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0085a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Matrix Approximation Experiments\n",
    "================================\n",
    "\n",
    "This module implements various matrix approximation techniques and runs experiments\n",
    "on matrices from the SuiteSparse Matrix Collection. It includes:\n",
    "- SVD-based approximation methods\n",
    "- CUR decomposition\n",
    "- Reinforcement Learning-based column selection\n",
    "- Evaluation metrics and visualization\n",
    "\n",
    "All operations include progress bars and color-coded logging for better tracking.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import colorama\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "\n",
    "\n",
    "# Initialize colorama\n",
    "colorama.init(autoreset=True)\n",
    "\n",
    "# Make sure directories exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# Matrix Approximation Methods\n",
    "# =============================================================================\n",
    "\n",
    "def deterministic_rank_approx(A, rank):\n",
    "    \"\"\"\n",
    "    Compute a deterministic low-rank approximation using SVD.\n",
    "\n",
    "    Args:\n",
    "        A (numpy.ndarray): Input matrix\n",
    "        rank (int): Target rank for the approximation\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Low-rank approximation of A\n",
    "    \"\"\"\n",
    "    log_debug(f\"Computing deterministic SVD rank-{rank} approximation\")\n",
    "    tqdm.write(f\"Computing SVD... (size: {A.shape})\")\n",
    "\n",
    "    with tqdm(total=3, desc=\"SVD Steps\") as pbar:\n",
    "        # Step 1: Compute SVD\n",
    "        U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 2: Truncate to rank\n",
    "        U_k = U[:, :rank]\n",
    "        s_k = s[:rank]\n",
    "        Vt_k = Vt[:rank, :]\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 3: Multiply back to get approximation\n",
    "        A_approx = U_k @ np.diag(s_k) @ Vt_k\n",
    "        pbar.update(1)\n",
    "\n",
    "    return A_approx\n",
    "\n",
    "def randomized_rank_approx(A, rank, n_oversample=10, n_iter=7):\n",
    "    \"\"\"\n",
    "    Compute a randomized low-rank approximation.\n",
    "\n",
    "    Args:\n",
    "        A (numpy.ndarray): Input matrix\n",
    "        rank (int): Target rank for the approximation\n",
    "        n_oversample (int): Oversampling parameter\n",
    "        n_iter (int): Number of power iterations\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Low-rank approximation of A\n",
    "    \"\"\"\n",
    "    log_debug(f\"Computing randomized SVD rank-{rank} approximation\")\n",
    "    m, n = A.shape\n",
    "    rank_target = min(rank + n_oversample, min(m, n))\n",
    "\n",
    "    steps = n_iter + 3  # Random projection, power iterations, and final SVD + reconstruction\n",
    "    with tqdm(total=steps, desc=\"Randomized SVD\") as pbar:\n",
    "        # Step 1: Random projection\n",
    "        Q = np.random.normal(size=(n, rank_target))\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 2: Power iteration\n",
    "        for _ in range(n_iter):\n",
    "            Q = A @ (A.T @ Q)\n",
    "            Q, _ = np.linalg.qr(Q, mode='reduced')\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Step 3: Project and compute SVD\n",
    "        B = Q.T @ A\n",
    "        U_B, s, Vt = np.linalg.svd(B, full_matrices=False)\n",
    "        U = Q @ U_B\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 4: Reconstruct\n",
    "        A_approx = U[:, :rank] @ np.diag(s[:rank]) @ Vt[:rank, :]\n",
    "        pbar.update(1)\n",
    "\n",
    "    return A_approx\n",
    "\n",
    "def cur_decomposition(A, rank, c_factor=5):\n",
    "    \"\"\"\n",
    "    Compute a CUR decomposition for low-rank approximation.\n",
    "\n",
    "    Args:\n",
    "        A (numpy.ndarray): Input matrix\n",
    "        rank (int): Target rank for the approximation\n",
    "        c_factor (int): Multiplication factor for number of columns/rows to sample\n",
    "\n",
    "    Returns:\n",
    "        tuple: (A_approx, C, U, R) - Approximated matrix and CUR components\n",
    "    \"\"\"\n",
    "    log_debug(f\"Computing CUR decomposition (rank-{rank})\")\n",
    "    m, n = A.shape\n",
    "\n",
    "    steps = 5  # Compute probabilities, sample columns, sample rows, compute U, reconstruct\n",
    "    with tqdm(total=steps, desc=\"CUR Decomposition\") as pbar:\n",
    "        # Step 1: Compute column norms for sampling probabilities\n",
    "        col_norms = np.sum(A**2, axis=0)\n",
    "        col_probs = col_norms / np.sum(col_norms)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 2: Sample columns\n",
    "        c_indices = np.random.choice(n, size=rank*c_factor, replace=False, p=col_probs)\n",
    "        C = A[:, c_indices]\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 3: Compute row norms for sampling probabilities using C\n",
    "        row_norms = np.sum(C**2, axis=1)\n",
    "        row_probs = row_norms / np.sum(row_norms)\n",
    "        r_indices = np.random.choice(m, size=rank*c_factor, replace=False, p=row_probs)\n",
    "        R = A[r_indices, :]\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 4: Compute intersection matrix U\n",
    "        W = A[r_indices, :][:, c_indices]\n",
    "        U = np.linalg.pinv(W)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Step 5: Compute approximation\n",
    "        A_approx = C @ U @ R\n",
    "        pbar.update(1)\n",
    "\n",
    "    return A_approx, C, U, R\n",
    "\n",
    "# =============================================================================\n",
    "# Reinforcement Learning Environment for Column Selection\n",
    "# =============================================================================\n",
    "\n",
    "class ColumnSelectionEnv:\n",
    "    \"\"\"\n",
    "    Environment for RL-based column selection.\n",
    "\n",
    "    This environment represents the task of selecting columns from a matrix\n",
    "    to create a low-rank approximation using CUR-like decomposition.\n",
    "    \"\"\"\n",
    "    def __init__(self, A, target_rank):\n",
    "        \"\"\"\n",
    "        Initialize the column selection environment.\n",
    "\n",
    "        Args:\n",
    "            A (numpy.ndarray): Input matrix\n",
    "            target_rank (int): Target rank for the approximation\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        self.A_norm = np.linalg.norm(A, 'fro')\n",
    "        self.m, self.n = A.shape\n",
    "        self.target_rank = target_rank\n",
    "        self.selected_columns = []\n",
    "        self.available_columns = list(range(self.n))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to initial state.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Initial state\n",
    "        \"\"\"\n",
    "        self.selected_columns = []\n",
    "        self.available_columns = list(range(self.n))\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Get current state representation.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: State representation (binary vector of selected columns)\n",
    "        \"\"\"\n",
    "        # Binary vector indicating which columns have been selected\n",
    "        state = np.zeros(self.n)\n",
    "        state[self.selected_columns] = 1\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action (select a column) and return new state, reward, done flag.\n",
    "\n",
    "        Args:\n",
    "            action (int): Column index to select\n",
    "\n",
    "        Returns:\n",
    "            tuple: (next_state, reward, done, info) - Standard RL step return\n",
    "        \"\"\"\n",
    "        if action in self.available_columns:\n",
    "            self.selected_columns.append(action)\n",
    "            self.available_columns.remove(action)\n",
    "\n",
    "            # Calculate current approximation error\n",
    "            if len(self.selected_columns) >= 2:  # Need at least 2 columns for meaningful approximation\n",
    "                C = self.A[:, self.selected_columns]\n",
    "                U = np.linalg.pinv(C)\n",
    "                A_approx = C @ U @ self.A\n",
    "                error = np.linalg.norm(self.A - A_approx, 'fro') / self.A_norm\n",
    "                reward = -error  # Negative error as reward\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            done = len(self.selected_columns) >= self.target_rank\n",
    "\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            return self._get_state(), -1, False, {}  # Penalty for invalid action\n",
    "\n",
    "class EnhancedColumnSelectionEnv(ColumnSelectionEnv):\n",
    "    \"\"\"\n",
    "    Enhanced environment for RL-based column selection with different state representations\n",
    "    and reward functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, A, target_rank, state_type='binary', reward_type='error'):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced column selection environment.\n",
    "\n",
    "        Args:\n",
    "            A (numpy.ndarray): Input matrix\n",
    "            target_rank (int): Target rank for the approximation\n",
    "            state_type (str): Type of state representation ('binary', 'error', 'combined')\n",
    "            reward_type (str): Type of reward function ('error', 'improvement', 'combined')\n",
    "        \"\"\"\n",
    "        super().__init__(A, target_rank)\n",
    "        self.state_type = state_type\n",
    "        self.reward_type = reward_type\n",
    "\n",
    "        # Pre-compute column leverage scores for state representation\n",
    "        U, _, _ = np.linalg.svd(A, full_matrices=False)\n",
    "        self.leverage_scores = np.sum(U[:, :min(A.shape)]**2, axis=1)\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Get enhanced state representation based on state_type.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: State representation\n",
    "        \"\"\"\n",
    "        if self.state_type == 'binary':\n",
    "            # Binary vector indicating which columns have been selected\n",
    "            state = np.zeros(self.n)\n",
    "            state[self.selected_columns] = 1\n",
    "            return state\n",
    "\n",
    "        elif self.state_type == 'error':\n",
    "            # State based on current approximation error for each column\n",
    "            state = np.zeros(self.n)\n",
    "            if not self.selected_columns:\n",
    "                # Initial state - use column norms\n",
    "                col_norms = np.sum(self.A**2, axis=0)\n",
    "                state = col_norms / np.max(col_norms)\n",
    "            else:\n",
    "                # Calculate error contribution for each column\n",
    "                C = self.A[:, self.selected_columns]\n",
    "                for col in range(self.n):\n",
    "                    if col in self.selected_columns:\n",
    "                        state[col] = 0  # Already selected\n",
    "                    else:\n",
    "                        # Calculate error reduction if this column is added\n",
    "                        temp_cols = self.selected_columns + [col]\n",
    "                        C_temp = self.A[:, temp_cols]\n",
    "                        U_temp = np.linalg.pinv(C_temp)\n",
    "                        A_approx = C_temp @ U_temp @ self.A\n",
    "                        error = np.linalg.norm(self.A - A_approx, 'fro') / self.A_norm\n",
    "                        state[col] = -error  # Negative error (higher is better)\n",
    "            return state\n",
    "\n",
    "        elif self.state_type == 'combined':\n",
    "            # Combine binary selection, column norms, and leverage scores\n",
    "            binary = np.zeros(self.n)\n",
    "            binary[self.selected_columns] = 1\n",
    "\n",
    "            col_norms = np.sum(self.A**2, axis=0) / np.sum(self.A**2)\n",
    "\n",
    "            # Compute current error contribution for each column\n",
    "            error_contrib = np.zeros(self.n)\n",
    "            if self.selected_columns:\n",
    "                C = self.A[:, self.selected_columns]\n",
    "                U = np.linalg.pinv(C)\n",
    "                P = C @ U  # Projection matrix\n",
    "                residual = self.A - P @ self.A\n",
    "                error_contrib = np.sum(residual**2, axis=0) / np.sum(residual**2)\n",
    "\n",
    "            # Concatenate features\n",
    "            return np.concatenate([binary, col_norms, error_contrib])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action with enhanced reward calculation.\n",
    "\n",
    "        Args:\n",
    "            action (int): Column index to select\n",
    "\n",
    "        Returns:\n",
    "            tuple: (next_state, reward, done, info)\n",
    "        \"\"\"\n",
    "        if action in self.available_columns:\n",
    "            self.selected_columns.append(action)\n",
    "            self.available_columns.remove(action)\n",
    "\n",
    "            # Calculate current approximation\n",
    "            done = len(self.selected_columns) >= self.target_rank\n",
    "\n",
    "            if len(self.selected_columns) >= 2:\n",
    "                C = self.A[:, self.selected_columns]\n",
    "                try:\n",
    "                    U = np.linalg.pinv(C)\n",
    "                    A_approx = C @ U @ self.A\n",
    "                    error = np.linalg.norm(self.A - A_approx, 'fro') / self.A_norm\n",
    "\n",
    "                    if self.reward_type == 'error':\n",
    "                        reward = -error\n",
    "                    elif self.reward_type == 'improvement':\n",
    "                        # Reward based on improvement from previous step\n",
    "                        if hasattr(self, 'prev_error'):\n",
    "                            reward = self.prev_error - error\n",
    "                        else:\n",
    "                            reward = 0\n",
    "                        self.prev_error = error\n",
    "                    elif self.reward_type == 'combined':\n",
    "                        # Combine error and diversity rewards\n",
    "                        base_reward = -error\n",
    "\n",
    "                        # Add diversity bonus\n",
    "                        if len(self.selected_columns) > 1:\n",
    "                            C_norm = np.linalg.norm(C, axis=0)\n",
    "                            C_normalized = C / C_norm\n",
    "                            coherence = np.abs(C_normalized.T @ C_normalized)\n",
    "                            np.fill_diagonal(coherence, 0)  # Ignore self-coherence\n",
    "                            diversity_penalty = np.mean(coherence)\n",
    "                            reward = base_reward - diversity_penalty\n",
    "                        else:\n",
    "                            reward = base_reward\n",
    "                    else:\n",
    "                        reward = -error  # Default\n",
    "                except np.linalg.LinAlgError:\n",
    "                    # Handle numerical instability\n",
    "                    reward = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            return self._get_state(), -1, False, {}  # Penalty for invalid action\n",
    "\n",
    "# =============================================================================\n",
    "# RL Agents\n",
    "# =============================================================================\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Enhanced Deep Q-Network for column selection.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256, 128]):\n",
    "        \"\"\"\n",
    "        Initialize the DQN network with configurable architecture.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of state space\n",
    "            action_dim (int): Dimension of action space\n",
    "            hidden_dims (list): Dimensions of hidden layers\n",
    "        \"\"\"\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Build a dynamic network based on hidden_dims\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(dim))  # Add batch normalization\n",
    "            prev_dim = dim\n",
    "\n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(prev_dim, action_dim)\n",
    "\n",
    "        # Apply weight initialization for better convergence\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights using Kaiming initialization.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (Q-values)\n",
    "        \"\"\"\n",
    "        # For batch size 1, adjust batch normalization dimensions\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        features = self.hidden_layers(x)\n",
    "        q_values = self.output_layer(features)\n",
    "        return q_values\n",
    "\n",
    "class LegacyDQNNetwork(nn.Module):\n",
    "    \"\"\"Old DQN network architecture for compatibility with saved models.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(LegacyDQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class LegacyA2CNetwork(nn.Module):\n",
    "    \"\"\"Old A2C network architecture for compatibility with saved models.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(LegacyA2CNetwork, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Actor (policy) network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "        # Critic (value) network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "def load_model_with_compatibility(model_path, state_dim, action_dim, model_type='dqn', device=None):\n",
    "    \"\"\"\n",
    "    Load model weights with backward compatibility for old model architectures.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved model\n",
    "        state_dim (int): Dimension of state space\n",
    "        action_dim (int): Dimension of action space\n",
    "        model_type (str): Type of model ('dqn' or 'a2c')\n",
    "        device (torch.device): Device to load the model to\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Loaded model with weights\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Try to load the state dict\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "        # Check which architecture the model was saved with by looking at keys\n",
    "        if model_type == 'dqn':\n",
    "            if 'fc1.weight' in state_dict:  # Old architecture\n",
    "                log_info(f\"Detected legacy DQN model format in {model_path}\")\n",
    "                model = LegacyDQNNetwork(state_dim, action_dim)\n",
    "                model.load_state_dict(state_dict)\n",
    "                return model\n",
    "            else:  # New architecture\n",
    "                model = DQNNetwork(state_dim, action_dim)\n",
    "                model.load_state_dict(state_dict)\n",
    "                return model\n",
    "        elif model_type == 'a2c':\n",
    "            if 'shared.0.weight' in state_dict:  # Old architecture\n",
    "                log_info(f\"Detected legacy A2C model format in {model_path}\")\n",
    "                model = LegacyA2CNetwork(state_dim, action_dim)\n",
    "                model.load_state_dict(state_dict)\n",
    "                return model\n",
    "            else:  # New architecture\n",
    "                model = A2CNetwork(state_dim, action_dim)\n",
    "                model.load_state_dict(state_dict)\n",
    "                return model\n",
    "    except Exception as e:\n",
    "        log_warning(f\"Failed to load model directly: {str(e)}\")\n",
    "        # If loading fails, return a new model with default initialization\n",
    "        if model_type == 'dqn':\n",
    "            log_info(f\"Creating new DQN model (ignoring saved weights)\")\n",
    "            return DQNNetwork(state_dim, action_dim)\n",
    "        else:\n",
    "            log_info(f\"Creating new A2C model (ignoring saved weights)\")\n",
    "            return A2CNetwork(state_dim, action_dim)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"Enhanced DQN agent for column selection with prioritized experience replay.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.99, epsilon_min=0.01,\n",
    "                 use_prioritized_replay=True):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced DQN agent.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of state space\n",
    "            action_dim (int): Dimension of action space\n",
    "            lr (float): Learning rate\n",
    "            gamma (float): Discount factor\n",
    "            epsilon (float): Initial exploration rate\n",
    "            epsilon_decay (float): Decay rate for exploration\n",
    "            epsilon_min (float): Minimum exploration rate\n",
    "            use_prioritized_replay (bool): Whether to use prioritized experience replay\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        # Use CUDA if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Create networks\n",
    "        self.q_network = DQNNetwork(state_dim, action_dim)\n",
    "        self.target_network = DQNNetwork(state_dim, action_dim)\n",
    "        self.q_network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        # Optimizer with learning rate scheduler\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, patience=20)\n",
    "        self.loss_fn = nn.SmoothL1Loss()  # Huber loss for stability\n",
    "\n",
    "        # Prioritized experience replay\n",
    "        self.use_prioritized_replay = use_prioritized_replay\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 20000  # Increased buffer size\n",
    "        self.batch_size = 128     # Increased batch size\n",
    "        self.update_freq = 5      # Update target network more frequently\n",
    "        self.steps = 0\n",
    "\n",
    "        # For prioritized experience replay\n",
    "        self.priorities = np.zeros(self.buffer_size)\n",
    "        self.alpha = 0.6          # Priority exponent\n",
    "        self.beta = 0.4           # Initial importance sampling weight\n",
    "        self.beta_increment = 0.001\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def select_action(self, state, available_actions):\n",
    "        \"\"\"\n",
    "        Select an action based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): Current state\n",
    "            available_actions (list): List of available actions\n",
    "\n",
    "        Returns:\n",
    "            int: Selected action\n",
    "        \"\"\"\n",
    "        if not available_actions:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Exploration\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                return np.random.choice(available_actions)\n",
    "\n",
    "            # Exploitation - convert state to tensor and ensure it's on the right device\n",
    "            state_tensor = torch.FloatTensor(state).to(self.device)\n",
    "\n",
    "            # Handle batch dimension for batch normalization\n",
    "            if state_tensor.dim() == 1:\n",
    "                state_tensor = state_tensor.unsqueeze(0)\n",
    "\n",
    "            # Set network to eval mode for inference\n",
    "            self.q_network.eval()\n",
    "\n",
    "            # Get Q-values, ensuring all tensor operations stay on the same device\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                # Move to CPU only after computation is complete\n",
    "                q_values = q_values.cpu().detach().numpy().flatten()\n",
    "\n",
    "            # Set network back to train mode\n",
    "            self.q_network.train()\n",
    "\n",
    "            # Mask unavailable actions with large negative values\n",
    "            mask = np.ones_like(q_values) * float('-inf')\n",
    "            for action in available_actions:\n",
    "                mask[action] = 0\n",
    "            masked_q_values = q_values + mask\n",
    "\n",
    "            # Choose action with highest Q-value among available actions\n",
    "            return np.argmax(masked_q_values)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in select_action: {e}\")\n",
    "            # Fallback to random selection\n",
    "            return np.random.choice(available_actions)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in replay buffer with priority.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): Current state\n",
    "            action (int): Action taken\n",
    "            reward (float): Reward received\n",
    "            next_state (numpy.ndarray): Next state\n",
    "            done (bool): Whether episode is done\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "            self.priorities[len(self.buffer) - 1] = self.max_priority\n",
    "        else:\n",
    "            # Replace random sample with lower priority\n",
    "            if self.use_prioritized_replay:\n",
    "                idx = np.random.choice(len(self.buffer), p=1.0 / (self.priorities[:len(self.buffer)] + 1e-10))\n",
    "            else:\n",
    "                idx = np.random.randint(0, len(self.buffer))\n",
    "\n",
    "            self.buffer[idx] = (state, action, reward, next_state, done)\n",
    "            self.priorities[idx] = self.max_priority\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"\n",
    "        Get a batch of experiences with prioritized sampling.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Batch of (states, actions, rewards, next_states, dones, indices, weights)\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        if self.use_prioritized_replay:\n",
    "            # Compute sampling probabilities from priorities\n",
    "            priorities = self.priorities[:len(self.buffer)]\n",
    "            probs = priorities ** self.alpha\n",
    "            probs = probs / np.sum(probs)\n",
    "\n",
    "            # Sample batch and compute importance sampling weights\n",
    "            indices = np.random.choice(len(self.buffer), self.batch_size, p=probs)\n",
    "            weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "            weights = weights / np.max(weights)  # Normalize weights\n",
    "\n",
    "            # Increase beta over time for convergence to unbiased updates\n",
    "            self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        else:\n",
    "            # Uniform sampling\n",
    "            indices = np.random.choice(len(self.buffer), self.batch_size)\n",
    "            weights = np.ones(self.batch_size)\n",
    "\n",
    "        # Get batch data\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.int64)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"\n",
    "        Update priorities in the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            indices (list): Indices of transitions\n",
    "            td_errors (numpy.ndarray): TD errors for the transitions\n",
    "        \"\"\"\n",
    "        if not self.use_prioritized_replay:\n",
    "            return\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Priority is proportional to absolute TD error plus small constant\n",
    "            # Extract single value properly to avoid deprecation warning\n",
    "            td_error = float(abs(td_errors[i].item()))\n",
    "            self.priorities[idx] = td_error + 1e-5\n",
    "            self.max_priority = max(self.max_priority, self.priorities[idx])\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Learn from prioritized experiences in replay buffer.\"\"\"\n",
    "        # Get batch\n",
    "        batch = self.get_batch()\n",
    "        if batch is None:\n",
    "            return 0  # No learning done\n",
    "\n",
    "        states, actions, rewards, next_states, dones, indices, weights = batch\n",
    "\n",
    "        # Convert to tensors and move to device\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Current Q values\n",
    "        current_q = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Target Q values with double DQN: select action using the online network\n",
    "        # but evaluate using the target network\n",
    "        with torch.no_grad():\n",
    "            # Select actions from online network\n",
    "            next_actions = self.q_network(next_states).argmax(dim=1, keepdim=True)\n",
    "            # Evaluate with target network\n",
    "            next_q = self.target_network(next_states).gather(1, next_actions)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "\n",
    "        # Compute TD errors for updating priorities\n",
    "        td_errors = target_q.detach() - current_q.detach()  # Detach both tensors\n",
    "\n",
    "        # Update priorities - make sure to detach tensor before converting to numpy\n",
    "        self.update_priorities(indices, td_errors.cpu().detach().numpy())\n",
    "\n",
    "        # Weighted loss to account for importance sampling\n",
    "        loss = (weights * self.loss_fn(current_q, target_q)).mean()\n",
    "\n",
    "        # Update network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "class A2CNetwork(nn.Module):\n",
    "    \"\"\"Enhanced Actor-Critic Network for column selection.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256, 128]):\n",
    "        \"\"\"\n",
    "        Initialize the A2C network with improved architecture.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of state space\n",
    "            action_dim (int): Dimension of action space\n",
    "            hidden_dims (list): Dimensions of hidden layers\n",
    "        \"\"\"\n",
    "        super(A2CNetwork, self).__init__()\n",
    "\n",
    "        # Build a dynamic shared network based on hidden_dims\n",
    "        shared_layers = []\n",
    "        prev_dim = state_dim\n",
    "\n",
    "        # Create shared feature extractor with batch normalization\n",
    "        for i, dim in enumerate(hidden_dims[:-1]):\n",
    "            shared_layers.append(nn.Linear(prev_dim, dim))\n",
    "            shared_layers.append(nn.ReLU())\n",
    "            shared_layers.append(nn.BatchNorm1d(dim))\n",
    "            prev_dim = dim\n",
    "\n",
    "        self.shared = nn.Sequential(*shared_layers)\n",
    "\n",
    "        # Actor (policy) network with separate final hidden layer\n",
    "        self.actor_hidden = nn.Sequential(\n",
    "            nn.Linear(prev_dim, hidden_dims[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dims[-1])\n",
    "        )\n",
    "        self.actor_output = nn.Linear(hidden_dims[-1], action_dim)\n",
    "\n",
    "        # Critic (value) network with separate final hidden layer\n",
    "        self.critic_hidden = nn.Sequential(\n",
    "            nn.Linear(prev_dim, hidden_dims[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dims[-1])\n",
    "        )\n",
    "        self.critic_output = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights using Kaiming initialization.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            tuple: (actor_output, critic_output) - Policy logits and value\n",
    "        \"\"\"\n",
    "        # For batch size 1, adjust batch normalization dimensions\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        shared_features = self.shared(x)\n",
    "\n",
    "        # Actor path\n",
    "        actor_features = self.actor_hidden(shared_features)\n",
    "        actor_output = self.actor_output(actor_features)\n",
    "\n",
    "        # Critic path\n",
    "        critic_features = self.critic_hidden(shared_features)\n",
    "        critic_output = self.critic_output(critic_features)\n",
    "\n",
    "        return actor_output, critic_output\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"Enhanced A2C agent for column selection with additional optimization features.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, entropy_coef=0.01, value_coef=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced A2C agent.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of state space\n",
    "            action_dim (int): Dimension of action space\n",
    "            lr (float): Learning rate\n",
    "            gamma (float): Discount factor\n",
    "            entropy_coef (float): Entropy regularization coefficient\n",
    "            value_coef (float): Value loss coefficient\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "\n",
    "        # Use CUDA if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Create A2C network\n",
    "        self.model = A2CNetwork(state_dim, action_dim)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Advanced optimizer with learning rate scheduler\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, eps=1e-5)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.5, patience=30, verbose=True)\n",
    "\n",
    "        # Track learning metrics\n",
    "        self.avg_reward = 0\n",
    "        self.total_updates = 0\n",
    "\n",
    "    def select_action(self, state, available_actions):\n",
    "        \"\"\"\n",
    "        Select an action based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): Current state\n",
    "            available_actions (list): List of available actions\n",
    "\n",
    "        Returns:\n",
    "            int: Selected action\n",
    "        \"\"\"\n",
    "        if not available_actions:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Convert state to tensor and ensure it's on the right device\n",
    "            state_tensor = torch.FloatTensor(state).to(self.device)\n",
    "\n",
    "            # Handle batch dimension for batch normalization\n",
    "            if state_tensor.dim() == 1:\n",
    "                state_tensor = state_tensor.unsqueeze(0)\n",
    "\n",
    "            # Set model to eval mode for inference to properly handle batch normalization\n",
    "            self.model.eval()\n",
    "\n",
    "            # Make sure model is on the correct device\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "            # Get policy logits and value - all on the same device\n",
    "            with torch.no_grad():\n",
    "                logits, _ = self.model(state_tensor)\n",
    "                # Move to CPU only after computation is complete\n",
    "                logits = logits.cpu().detach().numpy().flatten()  # Ensure 1D array\n",
    "\n",
    "            # Set model back to train mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Create action mask for unavailable actions\n",
    "            mask = np.ones_like(logits) * float('-inf')\n",
    "            for action in available_actions:\n",
    "                mask[action] = 0\n",
    "            masked_logits = logits + mask\n",
    "\n",
    "            # Safe softmax calculation\n",
    "            max_logit = np.max(masked_logits)\n",
    "            if np.isneginf(max_logit):  # All actions are masked\n",
    "                return np.random.choice(available_actions)\n",
    "\n",
    "            exp_logits = np.exp(masked_logits - max_logit)\n",
    "            probs = exp_logits / np.sum(exp_logits)\n",
    "\n",
    "            # Handle numerical issues\n",
    "            if np.isnan(probs).any() or np.sum(probs) < 1e-10:\n",
    "                print(\"Warning: Invalid probability distribution, using random action\")\n",
    "                return np.random.choice(available_actions)\n",
    "\n",
    "            # Exploration-exploitation trade-off: sometimes pick greedy, sometimes sample\n",
    "            if np.random.random() < 0.1:  # 10% of the time be greedy\n",
    "                return np.argmax(probs)\n",
    "            else:\n",
    "                try:\n",
    "                    return np.random.choice(self.action_dim, p=probs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in action sampling: {e}\")\n",
    "                    # Fallback if distribution isn't valid\n",
    "                    return np.random.choice(available_actions)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in select_action: {e}\")\n",
    "            # Fallback to random selection on any error\n",
    "            return np.random.choice(available_actions)\n",
    "\n",
    "    def update(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Update the A2C network using collected trajectories.\n",
    "\n",
    "        Args:\n",
    "            states (list): List of states\n",
    "            actions (list): List of actions\n",
    "            rewards (list): List of rewards\n",
    "            next_states (list): List of next states\n",
    "            dones (list): List of done flags\n",
    "\n",
    "        Returns:\n",
    "            tuple: (policy_loss, value_loss, entropy) - Loss metrics\n",
    "        \"\"\"\n",
    "        if not states:\n",
    "            return 0, 0, 0\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.int64)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        # Add reward normalization for stability\n",
    "        if len(rewards) > 1 and np.std(rewards) > 0:\n",
    "            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-10)\n",
    "\n",
    "        # Convert numpy arrays to tensors and move to device\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Get current policy logits and values\n",
    "        logits, values = self.model(states)\n",
    "\n",
    "        # Compute next state values for bootstrapping\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.model(next_states)\n",
    "            next_values = next_values.squeeze()\n",
    "\n",
    "        # Compute discounted returns and advantages\n",
    "        returns = rewards + self.gamma * next_values * (1 - dones)\n",
    "        advantages = returns - values.squeeze()\n",
    "\n",
    "        # Normalize advantages for stable learning\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        # Compute policy loss using the log-probability of actions\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        # Select log probs of taken actions\n",
    "        action_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute policy loss using advantages\n",
    "        policy_loss = -(action_log_probs * advantages).mean()\n",
    "\n",
    "        # Compute value loss\n",
    "        value_loss = F.mse_loss(values.squeeze(), returns)\n",
    "\n",
    "        # Compute entropy for regularization (encourages exploration)\n",
    "        entropy = -(probs * log_probs).sum(dim=1).mean()\n",
    "\n",
    "        # Compute total loss\n",
    "        total_loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "\n",
    "        # Update model\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Apply gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        self.total_updates += 1\n",
    "\n",
    "        # Convert tensor values to Python floats for return\n",
    "        return (\n",
    "            policy_loss.item(),\n",
    "            value_loss.item(),\n",
    "            entropy.item()\n",
    "        )\n",
    "\n",
    "    def update_scheduler(self, avg_reward):\n",
    "        \"\"\"Update learning rate scheduler based on performance.\"\"\"\n",
    "        self.scheduler.step(avg_reward)\n",
    "\n",
    "# =============================================================================\n",
    "# Training functions\n",
    "# =============================================================================\n",
    "\n",
    "def train_dqn(agent, env, num_episodes=500, early_stopping_patience=50, save_path=None):\n",
    "    \"\"\"\n",
    "    Train a DQN agent for column selection with advanced features like\n",
    "    early stopping and model checkpointing.\n",
    "\n",
    "    Args:\n",
    "        agent (DQNAgent): The DQN agent to train\n",
    "        env (ColumnSelectionEnv): The environment\n",
    "        num_episodes (int): Maximum number of episodes to train for\n",
    "        early_stopping_patience (int): Number of episodes to wait for improvement before stopping\n",
    "        save_path (str): Path to save the best model checkpoint, if None no checkpointing is done\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rewards_history, errors_history, best_error) - Training metrics and best performance\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    errors_history = []\n",
    "    losses = []\n",
    "\n",
    "    # For early stopping\n",
    "    best_error = float('inf')\n",
    "    best_reward = float('-inf')\n",
    "    best_weights = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Create progress bar for episodes\n",
    "    with tqdm(total=num_episodes, desc=\"Training DQN\") as pbar:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            episode_losses = []\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # Select and perform action\n",
    "                action = agent.select_action(state, env.available_columns)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Store experience\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                # Learn from experience\n",
    "                loss = agent.replay()\n",
    "                if loss is not None:\n",
    "                    episode_losses.append(loss)\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            # Calculate error at the end of episode\n",
    "            if env.selected_columns:\n",
    "                C = env.A[:, env.selected_columns]\n",
    "                U = np.linalg.pinv(C)\n",
    "                A_approx = C @ U @ env.A\n",
    "                error = np.linalg.norm(env.A - A_approx, 'fro') / env.A_norm\n",
    "                errors_history.append(error)\n",
    "            else:\n",
    "                error = 1.0\n",
    "                errors_history.append(error)\n",
    "\n",
    "            rewards_history.append(total_reward)\n",
    "            losses.append(np.mean(episode_losses) if episode_losses else 0)\n",
    "\n",
    "            # Update scheduler with negative error (higher is better)\n",
    "            if hasattr(agent, 'scheduler'):\n",
    "                agent.scheduler.step(-error)\n",
    "\n",
    "            # Check for best performance and save model if needed\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_reward = total_reward\n",
    "                patience_counter = 0\n",
    "\n",
    "                # Save best model\n",
    "                if save_path:\n",
    "                    best_weights = agent.q_network.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Update progress bar with useful information\n",
    "            pbar.set_postfix({\n",
    "                'reward': f'{total_reward:.4f}',\n",
    "                'error': f'{error:.4f}',\n",
    "                'epsilon': f'{agent.epsilon:.4f}',\n",
    "                'best': f'{best_error:.4f}',\n",
    "                'patience': f'{patience_counter}/{early_stopping_patience}'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Log periodically\n",
    "            if episode % 20 == 0:\n",
    "                log_info(f\"Episode {episode}/{num_episodes}, Reward: {total_reward:.4f}, \"\n",
    "                         f\"Error: {error:.4f}, Best: {best_error:.4f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                log_info(f\"Early stopping triggered after {episode+1} episodes. Best error: {best_error:.6f}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model weights if we have them\n",
    "    if best_weights is not None:\n",
    "        agent.q_network.load_state_dict(best_weights)\n",
    "        agent.target_network.load_state_dict(best_weights)\n",
    "\n",
    "    # Save the model if path is provided and not already saved\n",
    "    if save_path and best_weights is not None:\n",
    "        torch.save(best_weights, save_path)\n",
    "        log_success(f\"Best model saved to {save_path}\")\n",
    "\n",
    "    return rewards_history, errors_history, best_error\n",
    "\n",
    "def train_a2c(agent, env, num_episodes=500, update_interval=5, early_stopping_patience=50, save_path=None):\n",
    "    \"\"\"\n",
    "    Train an A2C agent for column selection with advanced features.\n",
    "\n",
    "    Args:\n",
    "        agent (A2CAgent): The A2C agent to train\n",
    "        env (ColumnSelectionEnv): The environment\n",
    "        num_episodes (int): Maximum number of episodes to train for\n",
    "        update_interval (int): How often to update the policy\n",
    "        early_stopping_patience (int): Number of episodes to wait for improvement before stopping\n",
    "        save_path (str): Path to save the best model checkpoint, if None no checkpointing is done\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rewards_history, errors_history, best_error) - Training metrics and best performance\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    errors_history = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "\n",
    "    # For early stopping\n",
    "    best_error = float('inf')\n",
    "    best_reward = float('-inf')\n",
    "    best_weights = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Device for computation\n",
    "    device = agent.device\n",
    "\n",
    "    # Create progress bar for episodes\n",
    "    with tqdm(total=num_episodes, desc=\"Training A2C\") as pbar:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "            total_reward = 0\n",
    "            episode_policy_losses = []\n",
    "            episode_value_losses = []\n",
    "            episode_entropies = []\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Select and perform action\n",
    "                action = agent.select_action(state, env.available_columns)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Store experience\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(float(done))\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                step += 1\n",
    "\n",
    "                # Update policy periodically or at end of episode\n",
    "                if step % update_interval == 0 or done:\n",
    "                    # Convert to numpy arrays and normalize rewards for stability\n",
    "                    if len(states) > 0:\n",
    "                        policy_loss, value_loss, entropy = agent.update(\n",
    "                            states, actions, rewards, next_states, dones)\n",
    "\n",
    "                        episode_policy_losses.append(policy_loss)\n",
    "                        episode_value_losses.append(value_loss)\n",
    "                        episode_entropies.append(entropy)\n",
    "\n",
    "                        # Clear buffers\n",
    "                        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "            # Calculate error at the end of episode\n",
    "            if env.selected_columns:\n",
    "                C = env.A[:, env.selected_columns]\n",
    "                U = np.linalg.pinv(C)\n",
    "                A_approx = C @ U @ env.A\n",
    "                error = np.linalg.norm(env.A - A_approx, 'fro') / env.A_norm\n",
    "                errors_history.append(error)\n",
    "            else:\n",
    "                error = 1.0\n",
    "                errors_history.append(error)\n",
    "\n",
    "            rewards_history.append(total_reward)\n",
    "\n",
    "            # Log losses if available\n",
    "            if episode_policy_losses:\n",
    "                policy_losses.append(np.mean(episode_policy_losses))\n",
    "                value_losses.append(np.mean(episode_value_losses))\n",
    "                entropies.append(np.mean(episode_entropies))\n",
    "\n",
    "            # Update scheduler based on negative error (higher is better)\n",
    "            agent.update_scheduler(-error)\n",
    "\n",
    "            # Check for best performance and save model if needed\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_reward = total_reward\n",
    "                patience_counter = 0\n",
    "\n",
    "                # Save best model\n",
    "                if save_path:\n",
    "                    best_weights = agent.model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Update progress bar with useful information\n",
    "            pbar.set_postfix({\n",
    "                'reward': f'{total_reward:.4f}',\n",
    "                'error': f'{error:.4f}',\n",
    "                'best': f'{best_error:.4f}',\n",
    "                'patience': f'{patience_counter}/{early_stopping_patience}'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Log periodically\n",
    "            if episode % 20 == 0:\n",
    "                log_info(f\"Episode {episode}/{num_episodes}, Reward: {total_reward:.4f}, \"\n",
    "                         f\"Error: {error:.4f}, Best: {best_error:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                log_info(f\"Early stopping triggered after {episode+1} episodes. Best error: {best_error:.6f}\")\n",
    "                break\n",
    "\n",
    "    # Restore best model weights if we have them\n",
    "    if best_weights is not None:\n",
    "        agent.model.load_state_dict(best_weights)\n",
    "\n",
    "    # Save the model if path is provided and not already saved\n",
    "    if save_path and best_weights is not None:\n",
    "        torch.save(best_weights, save_path)\n",
    "        log_success(f\"Best model saved to {save_path}\")\n",
    "\n",
    "    return rewards_history, errors_history, best_error\n",
    "\n",
    "# =============================================================================\n",
    "# Apply trained RL agents\n",
    "# =============================================================================\n",
    "\n",
    "def rl_column_selection_with_dqn(A, rank, agent):\n",
    "    \"\"\"\n",
    "    Apply trained DQN agent for column selection.\n",
    "\n",
    "    Args:\n",
    "        A (numpy.ndarray): Input matrix\n",
    "        rank (int): Target rank for approximation\n",
    "        agent (DQNAgent): Trained DQN agent\n",
    "\n",
    "    Returns:\n",
    "        tuple: (A_approx, selected_columns) - Approximated matrix and selected columns\n",
    "    \"\"\"\n",
    "    log_debug(f\"Applying DQN-based column selection (rank={rank})\")\n",
    "    env = ColumnSelectionEnv(A, rank)\n",
    "    state = env.reset()\n",
    "\n",
    "    with tqdm(total=rank, desc=\"DQN Column Selection\") as pbar:\n",
    "        for _ in range(rank):\n",
    "            action = agent.select_action(state, env.available_columns)\n",
    "            state, _, _, _ = env.step(action)\n",
    "            pbar.update(1)\n",
    "\n",
    "    selected_columns = env.selected_columns\n",
    "    C = A[:, selected_columns]\n",
    "    U = np.linalg.pinv(C)\n",
    "    A_approx = C @ U @ A\n",
    "\n",
    "    return A_approx, selected_columns\n",
    "\n",
    "def rl_column_selection_with_a2c(A, rank, agent, state_type='binary', reward_type='error'):\n",
    "    \"\"\"\n",
    "    Apply trained A2C agent for column selection.\n",
    "\n",
    "    Args:\n",
    "        A (numpy.ndarray): Input matrix\n",
    "        rank (int): Target rank for approximation\n",
    "        agent (A2CAgent): Trained A2C agent\n",
    "        state_type (str): Type of state representation\n",
    "        reward_type (str): Type of reward function\n",
    "\n",
    "    Returns:\n",
    "        tuple: (A_approx, selected_columns) - Approximated matrix and selected columns\n",
    "    \"\"\"\n",
    "    log_debug(f\"Applying A2C-based column selection (rank={rank})\")\n",
    "    env = EnhancedColumnSelectionEnv(A, rank, state_type, reward_type)\n",
    "    state = env.reset()\n",
    "\n",
    "    # Get the device from the agent\n",
    "    device = agent.device\n",
    "\n",
    "    with tqdm(total=rank, desc=\"A2C Column Selection\") as pbar:\n",
    "        for _ in range(rank):\n",
    "            # Make sure we have a valid available column\n",
    "            if not env.available_columns:\n",
    "                log_warning(\"No more available columns to select!\")\n",
    "                break\n",
    "\n",
    "            # Select action using the agent\n",
    "            action = agent.select_action(state, env.available_columns)\n",
    "\n",
    "            # Perform the action in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            pbar.update(1)\n",
    "\n",
    "    selected_columns = env.selected_columns\n",
    "    if len(selected_columns) < 2:\n",
    "        log_warning(f\"Only selected {len(selected_columns)} columns, which is insufficient. Using random columns instead.\")\n",
    "        selected_columns = np.random.choice(A.shape[1], size=min(rank, A.shape[1]), replace=False)\n",
    "\n",
    "    try:\n",
    "        C = A[:, selected_columns]\n",
    "        U = np.linalg.pinv(C)\n",
    "        A_approx = C @ U @ A\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        log_error(f\"Linear algebra error in A2C column selection: {str(e)}\")\n",
    "        # Fallback to random selection\n",
    "        log_warning(\"Falling back to random column selection\")\n",
    "        fallback_cols = np.random.choice(A.shape[1], size=min(rank, A.shape[1]), replace=False)\n",
    "        C = A[:, fallback_cols]\n",
    "        U = np.linalg.pinv(C)\n",
    "        A_approx = C @ U @ A\n",
    "\n",
    "    return A_approx, selected_columns\n",
    "\n",
    "# =============================================================================\n",
    "# Experiment functions\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_approximation(matrix_name, A, A_approx, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate a matrix approximation method.\n",
    "\n",
    "    Args:\n",
    "        matrix_name (str): Name of the matrix\n",
    "        A (numpy.ndarray): Original matrix\n",
    "        A_approx (numpy.ndarray): Approximated matrix\n",
    "        method_name (str): Name of the approximation method\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Relative Frobenius error\n",
    "    rel_frob_error = np.linalg.norm(A - A_approx, 'fro') / np.linalg.norm(A, 'fro')\n",
    "\n",
    "    # Spectral norm error\n",
    "    try:\n",
    "        spec_error = np.linalg.norm(A - A_approx, 2) / np.linalg.norm(A, 2)\n",
    "    except:\n",
    "        spec_error = np.nan\n",
    "\n",
    "    log_info(f\"[{matrix_name}] Method: {method_name}, Relative Error: {rel_frob_error:.6f}, \"\n",
    "             f\"Spectral Error: {spec_error:.6f}\")\n",
    "\n",
    "    return {\n",
    "        'matrix': matrix_name,\n",
    "        'method': method_name,\n",
    "        'rel_frob_error': rel_frob_error,\n",
    "        'spec_error': spec_error\n",
    "    }\n",
    "\n",
    "def run_matrix_experiments(matrices, target_rank=None, num_train_episodes=400):\n",
    "    \"\"\"\n",
    "    Run experiments with various matrix approximation methods.\n",
    "\n",
    "    Args:\n",
    "        matrices (dict): Dictionary of matrices to test\n",
    "        target_rank (int): Target rank for approximation, if None uses 10% of matrix size\n",
    "        num_train_episodes (int): Number of training episodes for RL methods\n",
    "\n",
    "    Returns:\n",
    "        list: List of evaluation results\n",
    "    \"\"\"\n",
    "    log_info(\"Starting matrix approximation experiments\")\n",
    "    results = []\n",
    "\n",
    "    # Select train and test matrices\n",
    "    if len(matrices) < 2:\n",
    "        log_error(\"Not enough matrices were loaded successfully.\")\n",
    "        return []\n",
    "\n",
    "    # Split into train and test sets\n",
    "    matrix_names = list(matrices.keys())\n",
    "    train_matrix_name = matrix_names[0]  # Use first matrix for training\n",
    "    test_matrix_names = matrix_names[1:]  # Use remaining matrices for testing\n",
    "\n",
    "    train_matrix = matrices[train_matrix_name]\n",
    "    test_matrices = {name: matrices[name] for name in test_matrix_names}\n",
    "\n",
    "    log_info(f\"Train matrix: {train_matrix_name}\")\n",
    "    log_info(f\"Test matrices: {test_matrix_names}\")\n",
    "\n",
    "    # Define target rank (e.g., 10% of matrix size)\n",
    "    if target_rank is None:\n",
    "        target_rank = max(5, min(train_matrix.shape[0] // 10, 80))  # Ensure rank is reasonable\n",
    "\n",
    "    log_info(f\"Target rank: {target_rank}\")\n",
    "\n",
    "    # Check for existing models\n",
    "    dqn_model_path = f\"models/dqn_{train_matrix_name}.pt\"\n",
    "    a2c_model_path = f\"models/a2c_{train_matrix_name}.pt\"\n",
    "    dqn_model_exists = os.path.exists(dqn_model_path)\n",
    "    a2c_model_exists = os.path.exists(a2c_model_path)\n",
    "\n",
    "    # Setup environment dimensions\n",
    "    env = ColumnSelectionEnv(train_matrix, target_rank)\n",
    "    state_dim = env.n\n",
    "    action_dim = env.n\n",
    "\n",
    "    # Train RL agents on the training matrix if models don't exist\n",
    "    log_info(\"Setting up RL agents...\")\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    log_info(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize enhanced DQN agent with prioritized experience replay\n",
    "    dqn_agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr=5e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.05,\n",
    "        use_prioritized_replay=True\n",
    "    )\n",
    "\n",
    "    # Train or load DQN model\n",
    "    if dqn_model_exists:\n",
    "        log_info(f\"Loading existing DQN model from {dqn_model_path}\")\n",
    "        # Load with compatibility check\n",
    "        loaded_model = load_model_with_compatibility(dqn_model_path, state_dim, action_dim, model_type='dqn', device=device)\n",
    "\n",
    "        # If legacy model, need to adapt our agent to use it\n",
    "        if isinstance(loaded_model, LegacyDQNNetwork):\n",
    "            log_info(\"Using legacy DQN model structure\")\n",
    "            dqn_agent.q_network = loaded_model\n",
    "            # Create target network of the same type\n",
    "            dqn_agent.target_network = LegacyDQNNetwork(state_dim, action_dim).to(device)\n",
    "        else:\n",
    "            dqn_agent.q_network = loaded_model\n",
    "\n",
    "        # Sync the target network\n",
    "        dqn_agent.target_network.load_state_dict(dqn_agent.q_network.state_dict())\n",
    "        log_success(f\"DQN model loaded successfully\")\n",
    "    else:\n",
    "        log_info(\"Training DQN agent with enhanced architecture...\")\n",
    "        train_dqn(\n",
    "            dqn_agent,\n",
    "            env,\n",
    "            num_episodes=num_train_episodes,\n",
    "            early_stopping_patience=50,\n",
    "            save_path=dqn_model_path\n",
    "        )\n",
    "        log_success(f\"DQN agent trained and saved to {dqn_model_path}\")\n",
    "\n",
    "    # Initialize enhanced A2C agent with combined state and reward\n",
    "    env = EnhancedColumnSelectionEnv(train_matrix, target_rank,\n",
    "                                     state_type='combined', reward_type='combined')\n",
    "    state_dim = env.n * 3 if env.state_type == 'combined' else env.n\n",
    "    a2c_agent = A2CAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        entropy_coef=0.02,  # Increased entropy coef for better exploration\n",
    "        value_coef=0.5\n",
    "    )\n",
    "\n",
    "    # Train or load A2C model\n",
    "    if a2c_model_exists:\n",
    "        log_info(f\"Loading existing A2C model from {a2c_model_path}\")\n",
    "        # Load with compatibility check\n",
    "        loaded_model = load_model_with_compatibility(a2c_model_path, state_dim, action_dim, model_type='a2c', device=device)\n",
    "\n",
    "        # If legacy model, need to adapt our agent to use it\n",
    "        if isinstance(loaded_model, LegacyA2CNetwork):\n",
    "            log_info(\"Using legacy A2C model structure\")\n",
    "\n",
    "        a2c_agent.model = loaded_model\n",
    "        log_success(f\"A2C model loaded successfully\")\n",
    "    else:\n",
    "        log_info(\"Training A2C agent with enhanced architecture...\")\n",
    "        train_a2c(\n",
    "            a2c_agent,\n",
    "            env,\n",
    "            num_episodes=num_train_episodes,\n",
    "            update_interval=5,\n",
    "            early_stopping_patience=50,\n",
    "            save_path=a2c_model_path\n",
    "        )\n",
    "        log_success(f\"A2C agent trained and saved to {a2c_model_path}\")\n",
    "\n",
    "    # Define main comparison methods (without CUR)\n",
    "    main_methods = {\n",
    "        \"Deterministic SVD\": lambda A, r: deterministic_rank_approx(A, r),\n",
    "        \"Randomized SVD\": lambda A, r: randomized_rank_approx(A, r),\n",
    "        \"RL-DQN\": lambda A, r: rl_column_selection_with_dqn(A, r, dqn_agent)[0],\n",
    "        \"RL-A2C\": lambda A, r: rl_column_selection_with_a2c(\n",
    "            A, r, a2c_agent, state_type='combined', reward_type='combined')[0],\n",
    "    }\n",
    "\n",
    "    # Define CUR separately for secondary comparison\n",
    "    cur_method = {\n",
    "        \"CUR\": lambda A, r: cur_decomposition(A, r)[0],\n",
    "    }\n",
    "\n",
    "    # Evaluate on the training matrix first\n",
    "    log_info(f\"Evaluating methods on training matrix: {train_matrix_name}\")\n",
    "    A_train = train_matrix\n",
    "\n",
    "    # Evaluate main methods\n",
    "    for method_name, method_fn in main_methods.items():\n",
    "        try:\n",
    "            log_debug(f\"Applying method {method_name} to matrix {train_matrix_name}\")\n",
    "            start_time = time.time()\n",
    "            A_approx = method_fn(A_train, target_rank)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            eval_result = evaluate_approximation(train_matrix_name, A_train, A_approx, method_name)\n",
    "            eval_result['time'] = elapsed_time\n",
    "            eval_result['train_matrix'] = True\n",
    "            results.append(eval_result)\n",
    "\n",
    "            log_success(f\"{method_name} on {train_matrix_name}: \"\n",
    "                       f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Failed {method_name} on {train_matrix_name}: {str(e)}\")\n",
    "\n",
    "    # Evaluate CUR separately\n",
    "    for method_name, method_fn in cur_method.items():\n",
    "        try:\n",
    "            log_debug(f\"Applying method {method_name} to matrix {train_matrix_name}\")\n",
    "            start_time = time.time()\n",
    "            A_approx = method_fn(A_train, target_rank)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            eval_result = evaluate_approximation(train_matrix_name, A_train, A_approx, method_name)\n",
    "            eval_result['time'] = elapsed_time\n",
    "            eval_result['train_matrix'] = True\n",
    "            results.append(eval_result)\n",
    "\n",
    "            log_success(f\"{method_name} on {train_matrix_name}: \"\n",
    "                       f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Failed {method_name} on {train_matrix_name}: {str(e)}\")\n",
    "\n",
    "    # Evaluate on test matrices\n",
    "    for test_name, A_test in test_matrices.items():\n",
    "        log_info(f\"Evaluating methods on test matrix: {test_name}\")\n",
    "\n",
    "        # Test main methods\n",
    "        for method_name, method_fn in main_methods.items():\n",
    "            try:\n",
    "                log_debug(f\"Applying method {method_name} to matrix {test_name}\")\n",
    "                start_time = time.time()\n",
    "                A_approx = method_fn(A_test, target_rank)\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                eval_result = evaluate_approximation(test_name, A_test, A_approx, method_name)\n",
    "                eval_result['time'] = elapsed_time\n",
    "                eval_result['train_matrix'] = False\n",
    "                results.append(eval_result)\n",
    "\n",
    "                log_success(f\"{method_name} on {test_name}: \"\n",
    "                           f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "            except Exception as e:\n",
    "                log_error(f\"Failed {method_name} on {test_name}: {str(e)}\")\n",
    "\n",
    "        # Test CUR separately\n",
    "        for method_name, method_fn in cur_method.items():\n",
    "            try:\n",
    "                log_debug(f\"Applying method {method_name} to matrix {test_name}\")\n",
    "                start_time = time.time()\n",
    "                A_approx = method_fn(A_test, target_rank)\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                eval_result = evaluate_approximation(test_name, A_test, A_approx, method_name)\n",
    "                eval_result['time'] = elapsed_time\n",
    "                eval_result['train_matrix'] = False\n",
    "                results.append(eval_result)\n",
    "\n",
    "                log_success(f\"{method_name} on {test_name}: \"\n",
    "                           f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "            except Exception as e:\n",
    "                log_error(f\"Failed {method_name} on {test_name}: {str(e)}\")\n",
    "\n",
    "    # Save results\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_csv('results/matrix_experiments.csv', index=False)\n",
    "    filemame= f\"results/mat_mehta{target_rank}.pkl\"\n",
    "    with open(filemame, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    log_success(\"Experiments completed and results saved\")\n",
    "    return results\n",
    "def run_matrix_experiments2(matrices, target_rank=None, num_train_episodes=400):\n",
    "    \"\"\"\n",
    "    Run experiments with various matrix approximation methods.\n",
    "\n",
    "    Args:\n",
    "        matrices (dict): Dictionary of matrices to test\n",
    "        target_rank (int): Target rank for approximation, if None uses 10% of matrix size\n",
    "        num_train_episodes (int): Number of training episodes for RL methods\n",
    "\n",
    "    Returns:\n",
    "        list: List of evaluation results\n",
    "    \"\"\"\n",
    "    log_info(\"Starting matrix approximation experiments\")\n",
    "    results = []\n",
    "\n",
    "    # Select train and test matrices\n",
    "    if len(matrices) < 2:\n",
    "        log_error(\"Not enough matrices were loaded successfully.\")\n",
    "        return []\n",
    "\n",
    "    # Split into train and test sets\n",
    "    matrix_names = list(matrices.keys())\n",
    "    train_matrix_name = matrix_names[0]  # Use first matrix for training\n",
    "    test_matrix_names = matrix_names[1:]  # Use remaining matrices for testing\n",
    "\n",
    "    train_matrix = matrices[train_matrix_name]\n",
    "    test_matrices = {name: matrices[name] for name in test_matrix_names}\n",
    "\n",
    "    log_info(f\"Train matrix: {train_matrix_name}\")\n",
    "    log_info(f\"Test matrices: {test_matrix_names}\")\n",
    "\n",
    "    # Define target rank (e.g., 10% of matrix size)\n",
    "    if target_rank is None:\n",
    "        target_rank = max(5, min(train_matrix.shape[0] // 10, 80))  # Ensure rank is reasonable\n",
    "\n",
    "    log_info(f\"Target rank: {target_rank}\")\n",
    "\n",
    "    # Check for existing models\n",
    "    dqn_model_path = f\"models/dqn_{train_matrix_name}.pt\"\n",
    "    a2c_model_path = f\"models/a2c_{train_matrix_name}.pt\"\n",
    "    dqn_model_exists = os.path.exists(dqn_model_path)\n",
    "    a2c_model_exists = os.path.exists(a2c_model_path)\n",
    "\n",
    "    # Setup environment dimensions\n",
    "    env = ColumnSelectionEnv(train_matrix, target_rank)\n",
    "    state_dim = env.n\n",
    "    action_dim = env.n\n",
    "\n",
    "    # Train RL agents on the training matrix if models don't exist\n",
    "    log_info(\"Setting up RL agents...\")\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    log_info(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize enhanced DQN agent with prioritized experience replay\n",
    "    dqn_agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr=5e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.05,\n",
    "        use_prioritized_replay=True\n",
    "    )\n",
    "\n",
    "    # Train or load DQN model\n",
    "    if dqn_model_exists:\n",
    "        log_info(f\"Loading existing DQN model from {dqn_model_path}\")\n",
    "        # Load with compatibility check\n",
    "        loaded_model = load_model_with_compatibility(dqn_model_path, state_dim, action_dim, model_type='dqn', device=device)\n",
    "\n",
    "        # If legacy model, need to adapt our agent to use it\n",
    "        if isinstance(loaded_model, LegacyDQNNetwork):\n",
    "            log_info(\"Using legacy DQN model structure\")\n",
    "            dqn_agent.q_network = loaded_model\n",
    "            # Create target network of the same type\n",
    "            dqn_agent.target_network = LegacyDQNNetwork(state_dim, action_dim).to(device)\n",
    "        else:\n",
    "            dqn_agent.q_network = loaded_model\n",
    "\n",
    "        # Sync the target network\n",
    "        dqn_agent.target_network.load_state_dict(dqn_agent.q_network.state_dict())\n",
    "        log_success(f\"DQN model loaded successfully\")\n",
    "    else:\n",
    "        log_info(\"Training DQN agent with enhanced architecture...\")\n",
    "        train_dqn(\n",
    "            dqn_agent,\n",
    "            env,\n",
    "            num_episodes=num_train_episodes,\n",
    "            early_stopping_patience=50,\n",
    "            save_path=dqn_model_path\n",
    "        )\n",
    "        log_success(f\"DQN agent trained and saved to {dqn_model_path}\")\n",
    "\n",
    "    # Initialize enhanced A2C agent with combined state and reward\n",
    "    env = EnhancedColumnSelectionEnv(train_matrix, target_rank,\n",
    "                                     state_type='combined', reward_type='combined')\n",
    "    state_dim = env.n * 3 if env.state_type == 'combined' else env.n\n",
    "    a2c_agent = A2CAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        entropy_coef=0.02,  # Increased entropy coef for better exploration\n",
    "        value_coef=0.5\n",
    "    )\n",
    "\n",
    "    # Train or load A2C model\n",
    "    if a2c_model_exists:\n",
    "        log_info(f\"Loading existing A2C model from {a2c_model_path}\")\n",
    "        # Load with compatibility check\n",
    "        loaded_model = load_model_with_compatibility(a2c_model_path, state_dim, action_dim, model_type='a2c', device=device)\n",
    "\n",
    "        # If legacy model, need to adapt our agent to use it\n",
    "        if isinstance(loaded_model, LegacyA2CNetwork):\n",
    "            log_info(\"Using legacy A2C model structure\")\n",
    "\n",
    "        a2c_agent.model = loaded_model\n",
    "        log_success(f\"A2C model loaded successfully\")\n",
    "    else:\n",
    "        log_info(\"Training A2C agent with enhanced architecture...\")\n",
    "        train_a2c(\n",
    "            a2c_agent,\n",
    "            env,\n",
    "            num_episodes=num_train_episodes,\n",
    "            update_interval=5,\n",
    "            early_stopping_patience=50,\n",
    "            save_path=a2c_model_path\n",
    "        )\n",
    "        log_success(f\"A2C agent trained and saved to {a2c_model_path}\")\n",
    "\n",
    "    # Define main comparison methods (without CUR)\n",
    "    main_methods = {\n",
    "        \"Deterministic SVD\": lambda A, r: deterministic_rank_approx(A, r),\n",
    "        \"Randomized SVD\": lambda A, r: randomized_rank_approx(A, r),\n",
    "        \"RL-DQN\": lambda A, r: rl_column_selection_with_dqn(A, r, dqn_agent)[0],\n",
    "        \"RL-A2C\": lambda A, r: rl_column_selection_with_a2c(\n",
    "            A, r, a2c_agent, state_type='combined', reward_type='combined')[0],\n",
    "    }\n",
    "\n",
    "    # Define CUR separately for secondary comparison\n",
    "    cur_method = {\n",
    "        \"CUR\": lambda A, r: cur_decomposition(A, r)[0],\n",
    "    }\n",
    "\n",
    "    # Evaluate on the training matrix first\n",
    "    log_info(f\"Evaluating methods on training matrix: {train_matrix_name}\")\n",
    "    A_train = train_matrix\n",
    "\n",
    "    # Evaluate main methods\n",
    "    for method_name, method_fn in main_methods.items():\n",
    "        try:\n",
    "            log_debug(f\"Applying method {method_name} to matrix {train_matrix_name}\")\n",
    "            start_time = time.time()\n",
    "            A_approx = method_fn(A_train, target_rank)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            eval_result = evaluate_approximation(train_matrix_name, A_train, A_approx, method_name)\n",
    "            eval_result['time'] = elapsed_time\n",
    "            eval_result['train_matrix'] = True\n",
    "            results.append(eval_result)\n",
    "\n",
    "            log_success(f\"{method_name} on {train_matrix_name}: \"\n",
    "                       f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Failed {method_name} on {train_matrix_name}: {str(e)}\")\n",
    "\n",
    "    # Evaluate CUR separately\n",
    "    for method_name, method_fn in cur_method.items():\n",
    "        try:\n",
    "            log_debug(f\"Applying method {method_name} to matrix {train_matrix_name}\")\n",
    "            start_time = time.time()\n",
    "            A_approx = method_fn(A_train, target_rank)\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            eval_result = evaluate_approximation(train_matrix_name, A_train, A_approx, method_name)\n",
    "            eval_result['time'] = elapsed_time\n",
    "            eval_result['train_matrix'] = True\n",
    "            results.append(eval_result)\n",
    "\n",
    "            log_success(f\"{method_name} on {train_matrix_name}: \"\n",
    "                       f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Failed {method_name} on {train_matrix_name}: {str(e)}\")\n",
    "\n",
    "    # Evaluate on test matrices\n",
    "    for test_name, A_test in test_matrices.items():\n",
    "        log_info(f\"Evaluating methods on test matrix: {test_name}\")\n",
    "\n",
    "        # Test main methods\n",
    "        for method_name, method_fn in main_methods.items():\n",
    "            try:\n",
    "                log_debug(f\"Applying method {method_name} to matrix {test_name}\")\n",
    "                start_time = time.time()\n",
    "                A_approx = method_fn(A_test, target_rank)\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                eval_result = evaluate_approximation(test_name, A_test, A_approx, method_name)\n",
    "                eval_result['time'] = elapsed_time\n",
    "                eval_result['train_matrix'] = False\n",
    "                results.append(eval_result)\n",
    "\n",
    "                log_success(f\"{method_name} on {test_name}: \"\n",
    "                           f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "            except Exception as e:\n",
    "                log_error(f\"Failed {method_name} on {test_name}: {str(e)}\")\n",
    "\n",
    "        # Test CUR separately\n",
    "        for method_name, method_fn in cur_method.items():\n",
    "            try:\n",
    "                log_debug(f\"Applying method {method_name} to matrix {test_name}\")\n",
    "                start_time = time.time()\n",
    "                A_approx = method_fn(A_test, target_rank)\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                eval_result = evaluate_approximation(test_name, A_test, A_approx, method_name)\n",
    "                eval_result['time'] = elapsed_time\n",
    "                eval_result['train_matrix'] = False\n",
    "                results.append(eval_result)\n",
    "\n",
    "                log_success(f\"{method_name} on {test_name}: \"\n",
    "                           f\"Error={eval_result['rel_frob_error']:.4f}, Time={elapsed_time:.4f}s\")\n",
    "            except Exception as e:\n",
    "                log_error(f\"Failed {method_name} on {test_name}: {str(e)}\")\n",
    "\n",
    "    # Save results\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_csv('results/matrix_experiments.csv', index=False)\n",
    "    filemame= f\"results/mat_nidhish{target_rank}.pkl\"\n",
    "    with open(filemame, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    # flush the file to ensure all data is written\n",
    "    f.flush()\n",
    "    # close the file\n",
    "    f.close()\n",
    "    log_success(\"Experiments completed and results saved\")\n",
    "    return results\n",
    "def plot_results(results, train_matrix_name):\n",
    "    \"\"\"\n",
    "    Plot experiment results.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of evaluation results\n",
    "        train_matrix_name (str): Name of the training matrix\n",
    "    \"\"\"\n",
    "    log_info(\"Generating result plots...\")\n",
    "\n",
    "    # Convert to DataFrame for easier plotting\n",
    "    if not isinstance(results, pd.DataFrame):\n",
    "        df = pd.DataFrame(results)\n",
    "    else:\n",
    "        df = results\n",
    "\n",
    "    # Separate CUR and main methods\n",
    "    main_df = df[df['method'] != 'CUR'].copy()\n",
    "    cur_df = df[df['method'].isin(['CUR', 'Deterministic SVD'])].copy()\n",
    "\n",
    "    # 1. Main Error comparison (without CUR)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x='matrix', y='rel_frob_error', hue='method', data=main_df)\n",
    "    plt.title(f'Error Comparison - Main Methods (Trained on {train_matrix_name})', fontsize=14)\n",
    "    plt.xlabel('Matrix', fontsize=12)\n",
    "    plt.ylabel('Relative Frobenius Error', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/error_comparison.png', dpi=300)\n",
    "\n",
    "    # 2. CUR Error comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x='matrix', y='rel_frob_error', hue='method', data=cur_df)\n",
    "    plt.title(f'Error Comparison - CUR vs SVD (Trained on {train_matrix_name})', fontsize=14)\n",
    "    plt.xlabel('Matrix', fontsize=12)\n",
    "    plt.ylabel('Relative Frobenius Error', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/cur_comparison.png', dpi=300)\n",
    "\n",
    "    # 3. Time comparison (all methods)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x='matrix', y='time', hue='method', data=df)\n",
    "    plt.title(f'Time Comparison (Trained on {train_matrix_name})', fontsize=14)\n",
    "    plt.xlabel('Matrix', fontsize=12)\n",
    "    plt.ylabel('Time (seconds)', fontsize=12)\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, which='both', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/time_comparison.png', dpi=300)\n",
    "    \n",
    "    # error comparision between cur and rl -dqn\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x='matrix', y='rel_frob_error', hue='method', data=cur_df)\n",
    "    plt.title(f'Error Comparison - CUR vs RL-DQN (Trained on {train_matrix_name})', fontsize=14)\n",
    "    plt.xlabel('Matrix', fontsize=12)\n",
    "    plt.ylabel('Relative Frobenius Error', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/cur_vs_rl_dqn_comparison.png', dpi=300)\n",
    "    \n",
    "\n",
    "    # 4. Standard error heatmap\n",
    "    methods = df['method'].unique()\n",
    "    matrices = df['matrix'].unique()\n",
    "\n",
    "    # Create a pivot table for the standard heatmap\n",
    "    heatmap_data = df.pivot_table(index='method', columns='matrix', values='rel_frob_error')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap=\"YlGnBu\", fmt=\".3f\")\n",
    "    plt.title(f'Error Heatmap (Trained on {train_matrix_name})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/error_heatmap.png', dpi=300)\n",
    "\n",
    "    # 5. Percentage error heatmap relative to Deterministic SVD\n",
    "    # Create pivot tables for each method and SVD\n",
    "    svd_errors = df[df['method'] == 'Deterministic SVD'].set_index('matrix')['rel_frob_error']\n",
    "\n",
    "    # Prepare percentage data\n",
    "    percentage_data = {}\n",
    "    for method in df['method'].unique():\n",
    "        if method != 'Deterministic SVD':\n",
    "            method_errors = df[df['method'] == method].set_index('matrix')['rel_frob_error']\n",
    "            percentages = {}\n",
    "            for matrix in method_errors.index:\n",
    "                if matrix in svd_errors.index:\n",
    "                    # Calculate percentage difference from SVD (positive means worse than SVD)\n",
    "                    percentages[matrix] = 100.0 * (method_errors[matrix] - svd_errors[matrix]) / svd_errors[matrix]\n",
    "            percentage_data[method] = percentages\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    percentage_df = pd.DataFrame(percentage_data).T\n",
    "\n",
    "    # Plot percentage heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)  # Red is positive (worse), Blue is negative (better)\n",
    "\n",
    "    # Create heatmap with percentage values\n",
    "    sns.heatmap(percentage_df, cmap=cmap, center=0, annot=True, fmt=\".1f\",\n",
    "                linewidths=.5, cbar_kws={'label': 'Error % relative to SVD'})\n",
    "\n",
    "    plt.title(f'Error Percentage Relative to Deterministic SVD (Trained on {train_matrix_name})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/relative_error_heatmap.png', dpi=300)\n",
    "\n",
    "    log_success(\"All plots saved in the results directory\")\n",
    "\n",
    "    return df  # Return the DataFrame for further analysis if needed\n",
    "\n",
    "# =============================================================================\n",
    "# Main function\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the experiments.\"\"\"\n",
    "    try:\n",
    "        log_info(\"Starting matrix approximation experiments\")\n",
    "\n",
    "        # Define which matrices to download\n",
    "        # Create a subset of oscil_dcop matrices to work with (selecting 13 out of 57)\n",
    "        target_matrices = [\n",
    "            f\"oscil_dcop_{i}\" for i in range(1, 14)  # Use first 13 matrices\n",
    "        ]\n",
    "\n",
    "        log_info(f\"Downloading target matrices: {target_matrices}\")\n",
    "        matrix_paths = download_all_matrices(subset=target_matrices)\n",
    "\n",
    "        if not matrix_paths:\n",
    "            log_error(\"Failed to download any matrices. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # Load the matrices\n",
    "        log_info(\"Loading matrices...\")\n",
    "        matrices = load_all_matrices(matrix_paths)\n",
    "\n",
    "        if not matrices:\n",
    "            log_error(\"Failed to load any matrices. Exiting.\")\n",
    "            return\n",
    "\n",
    "        log_success(f\"Successfully loaded {len(matrices)} matrices\")\n",
    "\n",
    "        # Run experiments\n",
    "        target_rank =  10 # Using 40 as the target rank as requested\n",
    "        results = run_matrix_experiments(matrices, target_rank=target_rank, num_train_episodes=400)\n",
    "\n",
    "        # Plot results\n",
    "        train_matrix_name = list(matrices.keys())[0]  # First matrix was used for training\n",
    "        plot_results(results, train_matrix_name)\n",
    "\n",
    "        log_success(\"Experiments completed successfully!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        log_warning(\"Experiments interrupted by user\")\n",
    "    except Exception as e:\n",
    "        log_error(f\"Error running experiments: {str(e)}\")\n",
    "        import traceback\n",
    "        log_error(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
